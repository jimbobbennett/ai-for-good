<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>LAB GUIDE – includes</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://opsgilitylabs.blob.core.windows.net/lablayout/labstyles.css"></link>
</head>
<body>
<h2 id="exercise-3-analyze-data-with-stream-analytics-and-azure-functions">Exercise 3: Analyze Data with Stream Analytics and Azure Functions</h2>
<p><a href="https://azure.microsoft.com/services/stream-analytics/">Azure Stream Analytics</a> is a cloud-based service for ingesting high-velocity data streaming from devices, sensors, applications, Web sites, and other data sources and analyzing that data in real time. It supports a <a href="https://msdn.microsoft.com/library/azure/dn834998.aspx">SQL-like query language</a> that works over dynamic data streams and makes analyzing constantly changing data no more difficult than performing queries on static data stored in traditional databases. With Azure Stream Analytics, you can set up jobs that analyze incoming data for anomalies or information of interest and record the results, present notifications on dashboards, or even fire off alerts to mobile devices. And all of it can be done at low cost and with a minimum of effort.</p>
<p>Scenarios for the application of real-time data analytics are many and include fraud detection, identity-theft protection, optimizing the allocation of resources (think of an Uber-like transportation service that sends drivers to areas of increasing demand <em>before</em> that demand peaks), click-stream analysis on Web sites, and countless others. Having the ability to process data <em>as it comes in</em> rather than waiting until after it has been aggregated offers a competitive advantage to businesses that are agile enough to make adjustments on the fly.</p>
<p>In this lab, the second of four in a series, you will create an Azure Stream Analytics job and connect it to the IoT hub you created in the previous exercise. Then you will write an <a href="https://azure.microsoft.com/services/functions/">Azure Function</a> to receive the output, and use the two of them together to analyze data streaming in from a simulated camera array.</p>
<p><img src="images/road-map-2.png" /></p>
<h3 id="objectives">Objectives</h3>
<p>In this exercise, you will learn how to:</p>
<ul>
<li>Create a Stream Analytics job and test queries on sample data streams</li>
<li>Run a Stream Analytics job and perform queries on live data streams</li>
<li>Use an Azure IoT hub as a Stream Analytics input</li>
<li>Use an Azure Function as a Stream Analytics output</li>
</ul>
<h3 id="time-estimate">Time Estimate</h3>
<ul>
<li>30 minutes</li>
</ul>
<h2 id="task-1-create-a-stream-analytics-job">Task 1: Create a Stream Analytics job</h2>
<p>In this task, you will use the Azure Portal to create a Stream Analytics job and connect it to the IoT hub you created in the previous exercise.</p>
<ol type="1">
<li><p>From your LABVM, open the Azure Portal and log in if you haven’t already.</p></li>
<li><p>Click <strong>+ Create a resource</strong>, followed by <strong>Internet of Things</strong> and <strong>Stream Analytics job</strong>.</p>
<p><img src="images/new-stream-analytics-job.png" /></p></li>
<li><p>Name the job <strong>polar-bear-analytics</strong> and place it in the <strong>streaminglab-rg</strong> resource group that you created in the previous lab. Specify <strong>South Central US</strong> as the location. (That’s important, because your IoT hub is in the same region, and while you are not charged for data that moves within a data center, you typically <em>are</em> charged for data that moves <em>between</em> data centers. In addition, locating services that are connected to each other in the same region reduces latency.) Make sure <strong>Hosting environment</strong> is set to <strong>Cloud</strong>, and then click the <strong>Create</strong> button.</p>
<p><img src="images/create-stream-analytics-job.png" /></p></li>
<li><p>Open the “streaminglab-rg” resource group and click <strong>polar-bear-analytics</strong> to open the Stream Analytics job in the portal. If the Stream Analytics job doesn’t appear in the resource group, click the <strong>Refresh</strong> button at the top of the blade until it does.</p>
<p><img src="images/open-stream-analytics-job.png" /></p></li>
<li><p>Click the <strong>Inputs</strong> tile to add an input to the Stream Analytics job.</p>
<p><img src="images/add-input-1.png" /></p></li>
<li><p>Click <strong>+ Add stream input</strong>, then select <strong>IoT Hub</strong>.</p>
<p><img src="images/2018-05-21-12-38-56.png" /></p></li>
<li><p>In the dialog that opens, enter <strong>CameraInput</strong> for the <strong>Input alias</strong> box. Make sure <strong>Select IoT Hub from your subscriptions</strong> is selected and that your IoT Hub is selected under <strong>IoT Hub</strong>. Accept the defaults as in the example below, then click the <strong>Save</strong> button at the bottom of the blade.</p>
<p><img src="images/2018-05-21-12-45-59.png" /></p></li>
</ol>
<p>After a few moments, the new input — “CameraInput” — appears in the list of inputs for the Stream Analytics job. This is the only input you will create, but be aware that you can add any number of inputs to a Stream Analytics job. In the <a href="https://msdn.microsoft.com/library/azure/dn834998.aspx">Stream Analytics Query Language</a>, each input is treated as a separate data source similar to tables in a relational database. The query language is extremely expressive, even allowing input streams to be joined in a manner similar to joining database tables.</p>
<h2 id="task-2-prepare-a-query-and-test-with-sample-data">Task 2: Prepare a query and test with sample data</h2>
<p>The heart of a Stream Analytics job is the query that extracts information from the data stream. It is always advisable to test a query using sample data before deploying it against a live data stream, because with sample data, you can verify that a known set of inputs produces the expected outputs. In this task, you will enter a query into the Stream Analytics job you created in the previous task and test it with sample data.</p>
<ol type="1">
<li><p>Return to the Stream Analytics job in the portal and click <strong>Query</strong>.</p>
<p><img src="images/2018-10-19-12-23-18.png" /></p></li>
<li><p>Select the <strong>CameraInput</strong> input and then select <strong>Upload sample input</strong> from the Input preview tab.</p>
<p><img src="images/2019-10-20-14-33-01.png" /></p></li>
<li><p>Click the <strong>folder</strong> icon on the right and select the file named <strong>C:\OpsgilityTraining\SampleData\sample-data.json.</strong> Then click <strong>OK</strong> to upload the file.</p></li>
<li><p>When the upload is complete, enter the following query into the query window. Then click the <strong>Test query</strong> button to execute it:</p>
<pre class="sql"><code>SELECT 
    deviceId,
    latitude,
    longitude,
    url,
    timestamp,
    EventProcessedUtcTime,
    PartitionId,
    EventEnqueuedUtcTime
FROM CameraInput</code></pre>
<p><img src="images/2019-10-20-14-32-25.png" /></p></li>
<li><p>Confirm that you see the output pictured below. The test data contains 50 rows, each representing an event transmitted to the IoT hub by one of the cameras in the camera array. <strong>deviceid</strong> is the camera’s device ID, <strong>latitude</strong> and <strong>longitude</strong> specify the camera’s location, <strong>url</strong> is the URL of the blob containing the picture that was taken, and <strong>timestamp</strong> is the time at which the picture was taken. The other fields were added by Azure.</p>
<p><img src="images/2019-10-20-14-34-28.png" /></p></li>
<li><p>One of the key features of the Stream Analytics Query Language is its ability to group results using windows of time whose length you specify. Windowing is enacted by using the keywords <a href="https://msdn.microsoft.com/library/azure/dn835055.aspx">TumblingWindow</a>, <a href="https://msdn.microsoft.com/library/azure/dn835041.aspx">HoppingWindow</a>, <a href="https://msdn.microsoft.com/library/azure/dn835051.aspx">SlidingWindow</a> and <a href="https://msdn.microsoft.com/en-us/azure/stream-analytics/reference/session-window-azure-stream-analytics">SessionWindow</a> in a GROUP BY clause. To demonstrate, execute the following query to count the number of times the cameras were triggered each minute:</p>
<pre class="sql"><code>SELECT System.Timestamp as [Time Ending],
    COUNT(*) AS [Times Triggered]
FROM CameraInput TIMESTAMP BY timestamp
GROUP BY TumblingWindow(n, 1)</code></pre>
<blockquote>
<p><a href="https://msdn.microsoft.com/library/azure/mt573293.aspx">TIMESTAMP BY</a> is an important element of the Stream Analytics Query Language. If it was omitted from the query above, you would be querying for the number of events that arrived <em>at the event hub</em> each minute rather than the number of events that occurred at the camera locations. TIMESTAMP BY allows you to specify a field in the input stream as the event time.</p>
</blockquote></li>
<li><p>Confirm that you see the output below:</p>
<p><img src="images/2019-10-20-14-36-18.png" /></p></li>
<li><p>Now it’s time to check for two photos snapped by the same camera within 10 seconds. <strong>This is the query that you will use against a live data stream</strong>. The assumption is that since polar bears tend to move rather slowly, we will ignore pictures taken more than 10 seconds apart, but if the same camera snaps two pictures within 10 seconds, it is worth examining them to see if one of them contains a polar bear.</p>
<p>Enter the following query and click <strong>Test query</strong> to execute it:</p>
<pre class="sql"><code>SELECT C1.deviceId, C1.latitude, C1.longitude, C1.url, C1.timestamp
FROM CameraInput C1 TIMESTAMP BY timestamp
JOIN CameraInput C2 TIMESTAMP BY timestamp
ON C1.deviceId = C2.deviceId
AND DATEDIFF(ss, C1, C2) BETWEEN 0 AND 10
AND C1.Timestamp != C2.Timestamp</code></pre></li>
<li><p>This time the output should contain six rows, each representing two photographs taken by the same camera within 10 seconds and containing the URL of one of the pictures.</p>
<blockquote>
<p>If you wanted to include <em>both</em> URLs in the output, how would you modify the query to do it?</p>
</blockquote>
<p><img src="images/2019-10-20-14-38-46.png" /></p></li>
</ol>
<p>Finish up by clicking the <strong>Save query</strong> button at the top of the blade to save the query. This will be the query that’s executed when you run the Stream Analytics job.</p>
<h2 id="task-3-direct-output-to-an-azure-function">Task 3: Direct output to an Azure Function</h2>
<p>The query that you tested in the previous task employs simple logic: if the same camera snaps two pictures within 10 seconds, there <em>might</em> be a polar bear. But the ultimate goal is to determine with a great deal of confidence whether there really <em>is</em> a polar bear. That means supplementing Stream Analytics with machine learning.</p>
<p>One way to connect a Stream Analytics job to a machine-learning model running in the cloud is to use an <a href="https://azure.microsoft.com/services/functions/">Azure Function</a> as a Stream Analytics output. The function, which is invoked each time Stream Analytics produces an output, can then call out to the machine-learning model. In this task, you will write an Azure Function, connect it to Stream Analytics, and stub it out so you can verify that it’s being called.</p>
<ol type="1">
<li><p>In the Azure Portal, click <strong>+ Create a resource</strong>, followed by <strong>Compute</strong> and <strong>Function App</strong>.</p>
<p><img src="images/new-function-app.png" /></p></li>
<li><p>Enter the following configurations and then click <strong>Next: Hosting</strong>:</p>
<ul>
<li><p>Subscription: <em>Accept the default</em></p></li>
<li><p>Resource Group: <strong>streaminglab-rg</strong></p></li>
<li><p>Function App name: <strong><em>unique name for your function</em></strong></p></li>
<li><p>Publish: <strong>Code</strong></p></li>
<li><p>Runtime stack: <strong>Node.js</strong></p></li>
<li><p>Region: <em>The region you are using for this lab</em></p></li>
</ul>
<blockquote>
<p><strong>Note</strong>: When you create an Azure Function App, you can choose from two hosting plans: Consumption plan or App Service plan. The former is cheaper because you only pay when the function executes. But with Consumption plan, the function might not execute for several minutes after it’s called. With App Service plan, you pay more, but the function runs immediately.</p>
</blockquote>
<p><img src="images/2020-01-29-23-44-23.png" /></p></li>
<li><p>On the hosting tab, set the following configurations and then click <strong>Review + create</strong> then <strong>Create</strong>.</p>
<ul>
<li><p>Storage account: <em>accept the default</em></p></li>
<li><p>Operating system: <strong>Windows</strong></p></li>
<li><p>Plan type: <strong>App service plan</strong></p></li>
<li><p>Windows Plan: <em>accept the default</em></p></li>
<li><p>Sku and size: <strong>Standard S1</strong></p></li>
</ul></li>
<li><p>Wait for your deployment to complete and then click the <strong>Go to resource</strong> button to navigate to your function app.</p></li>
<li><p>On the <strong>Platform features</strong> tab, select <strong>SSL</strong> under the Networking menu.</p>
<p><img src="images/2018-10-19-15-30-30.png" /></p></li>
<li><p>Change the minimum TLS version to <strong>1.0</strong>.</p>
<p><img src="images/2018-10-19-15-29-46.png" /></p></li>
<li><p>Return to the function app page and hover over <strong>Functions</strong> then click the <strong>+</strong> sign (you may have to scroll to the left). Select <strong>In-portal</strong> for the development environment. Then click the <strong>Continue</strong> button.</p>
<p><img src="images/2018-10-19-17-55-33.png" /></p></li>
<li><p>Select <strong>Webhook + API</strong>, and then click <strong>Create</strong>.</p>
<p><img src="images/2018-10-19-17-56-44.png" /></p></li>
<li><p>Replace the code shown in the code editor with the statements below. Then click <strong>Save</strong>, followed by <strong>Run</strong>.</p>
<pre class="javascript"><code>module.exports = function (context, req) {
    context.log(req.rawBody);
    context.done();
};</code></pre>
<p><img src="images/save-and-run.png" /></p></li>
<li><p>Back on the function app blade, expand <strong>HttpTrigger1</strong> and click the <strong>Manage</strong> tab. Copy the <strong>default</strong> function key value to notepad.</p>
<p><img src="images/ManageKey.png" /></p></li>
<li><p>Return to the Stream Analytics job in the portal and click <strong>Outputs</strong>.</p>
<p><img src="images/add-output-1.png" /></p></li>
<li><p>Click <strong>+ Add</strong>, then choose <strong>Azure Function</strong> from the list.</p>
<p><img src="images/2018-03-30-13-53-12.png" /></p></li>
<li><p>Name the output “FunctionOutput.” Choose <strong>Provide azure function settings manually</strong> and enter the names of the function app and function (<strong>HttpTrigger1</strong>) created earlier in this exercise. Paste in the key copied earlier as well. Then click <strong>Save</strong>.</p>
<p><img src="images/2018-03-30-13-58-27.png" /></p>
<blockquote>
<p>Just as a Stream Analytics job will accept multiple inputs, it supports multiple outputs, too. In addition to passing the output to an Azure Function, you could easily add outputs to log the output from the job in an Azure SQL database, a Cosmos DB database, blob storage, and other locations.</p>
</blockquote></li>
<li><p>Wait for the output to appear in the list of outputs, indicating that it has been successfully added to the Stream Analytics job. Then click <strong>Query</strong> under <strong>Job topology</strong> on the left and modify the query you wrote in the previous task to include an <code>INTO</code> clause (line 2 below) that directs query results to the output you just added:</p>
<pre class="sql"><code>SELECT C1.deviceId, C1.latitude, C1.longitude, C1.url, C1.timestamp
INTO FunctionOutput
FROM CameraInput C1 TIMESTAMP BY timestamp
JOIN CameraInput C2 TIMESTAMP BY timestamp
ON C1.deviceId = C2.deviceId
AND DATEDIFF(ss, C1, C2) BETWEEN 0 AND 10
AND C1.Timestamp != C2.Timestamp</code></pre></li>
<li><p>Click <strong>Save query</strong> to save the query. Then return to the Stream Analytics job <strong>Overview</strong> blade and click the <strong>Start</strong> button to start it.</p>
<p><img src="images/start-stream-analytics-job-1.png" /></p></li>
<li><p>Make sure <strong>Job output start time</strong> is set to <strong>Now</strong>, and then click <strong>Start</strong> to start the run.</p>
<p><img src="images/start-stream-analytics-job-2.png" /></p></li>
</ol>
<p>The job will take a couple of minutes to start, but you don’t have to wait. Proceed to the next task and start generating events for Stream Analytics to ingest.</p>
<h2 id="task-4-analyze-a-live-data-stream">Task 4: Analyze a live data stream</h2>
<p>In this task, you will use Node.js to stream events from the simulated camera array that you registered previously in the lab. Then you will check the log output from the Azure Function to verify that the events are being received by IoT hub, input to Stream Analytics, and output to the function.</p>
<ol type="1">
<li><p>Return to the project directory that you created in the previous exercise and create a file in it named <strong>run.js</strong>. Paste the following code into the file:</p>
<pre class="javascript"><code>&#39;use strict&#39;;

var iotHubName = &#39;HUB_NAME&#39;;
var storageAccountName = &#39;ACCOUNT_NAME&#39;;
var storageAccountKey = &#39;ACCOUNT_KEY&#39;;

class Camera {
    constructor(id, latitude, longitude, key, files) {
        this._id = id;
        this._latitude = latitude;
        this._longitude = longitude;
        this._key = key;
        this._files = files.slice(0);
        this._ready = false;
    }

    get id() {
        return this._id;
    }

    connect(iotHubName, storageAccountName, storageAccountKey, callback) {
        // Connect to blob storage
        var azure = require(&#39;azure-storage&#39;);
        this._storageAccountName = storageAccountName;
        this._blobService = azure.createBlobService(storageAccountName, storageAccountKey);

        // Connect to the IoT hub
        var connectionString = &#39;HostName=&#39; + iotHubName + &#39;.azure-devices.net;DeviceId=&#39; + this._id + &#39;;SharedAccessKey=&#39; + this._key;
        var clientFromConnectionString = require(&#39;azure-iot-device-mqtt&#39;).clientFromConnectionString;
        this._client = clientFromConnectionString(connectionString);

        this._client.open((err) =&gt; {
            if (!err) {
                this._ready = true;
            }

            callback(this._ready);
        });
    }

    start() {
        // Register first callback for 5 to 60 seconds
        setTimeout(this.timer, (Math.random() * 30000) + 1000, this);
    }

    timer(self) {
        if (self._ready === true) {
            // &quot;Trigger&quot; the camera with a random photo
            var index = Math.floor(Math.random() * self._files.length);
            self.trigger(self._files[index], (err, result) =&gt; {});

            // Register another callback for 5 to 60 seconds
            setTimeout(self.timer, (Math.random() * 55000) + 5000, self);
        }
    }

    trigger(imageFileName, callback) {
        if (this._ready === true) {
            // Upload the image to blob storage
            this.upload(imageFileName, (err, result) =&gt; {
                if (err) {
                    callback(err, result);
                }
                else {
                    // Send an event to the IoT hub
                    this.send(imageFileName, (err, result) =&gt; {
                        console.log(this._id + &#39;: https://&#39; + this._storageAccountName + &#39;.blob.core.windows.net/photos/&#39; + imageFileName);
                        callback(err, result);
                    });
                }
            });
        }
    }

    upload(imageFileName, callback) {
        this._blobService.createBlockBlobFromLocalFile(&#39;photos&#39;, imageFileName, &#39;photos/&#39; + imageFileName, (err, result) =&gt; {
            callback(err, result);
        });
    }

    send(imageFileName, callback) {
        var Message = require(&#39;azure-iot-device&#39;).Message;

        var data = {
            &#39;deviceId&#39; : this._id,
            &#39;latitude&#39; : this._latitude,
            &#39;longitude&#39; : this._longitude,
            &#39;url&#39; : &#39;https://&#39; + this._storageAccountName + &#39;.blob.core.windows.net/photos/&#39; + imageFileName,
            &#39;timestamp&#39; : new Date().toISOString()
        };

        var message = new Message(JSON.stringify(data));

        this._client.sendEvent(message, (err, result) =&gt; {
            callback(err, result);
        });        
    }
}

// Load image file names
var fs = require(&#39;fs&#39;);

fs.readdir(&#39;photos&#39;, (err, files) =&gt; {
    // Create an array of cameras
    var cameras = JSON.parse(fs.readFileSync(&#39;cameras.json&#39;, &#39;utf8&#39;)).map(
        camera =&gt; new Camera(
            camera.deviceId,
            camera.latitude,
            camera.longitude,
            camera.key,
            files
        )
    );

    // Start the cameras
    cameras.forEach(camera =&gt; {
        camera.connect(iotHubName, storageAccountName, storageAccountKey, status =&gt; {
            if (status === true) {
                console.log(camera.id + &#39; connected&#39;);
                camera.start();
            }
            else {
                console.log(camera.id + &#39; failed to connect&#39;);
            }
        })
    });
});</code></pre>
<p>This code uses the new <a href="http://es6-features.org/#ClassDefinition">class support</a> in ECMAScript 6 (ES6) to define a class named <code>Camera</code>. Then it creates 10 <code>Camera</code> instances and starts them running. Each camera object connects to the IoT hub securely using an access key obtained from <strong>cameras.json</strong>, and then uses a random timer to transmit events every 5 to 60 seconds. Each event that is transmitted includes the camera’s ID, latitude, and longitude, as well as an image URL and a timestamp. The URL refers to an image that the camera uploaded to blob storage before firing the event. Images are randomly selected from the files in the project directory’s “photos” subdirectory.</p></li>
<li><p>Replace HUB_NAME on line 3 with the name of the IoT hub that you created in the previous exercise, ACCOUNT_NAME on line 4 with the name of the storage account that you created in the same exercise, and ACCOUNT_KEY on line 5 with the storage account’s access key (The <strong>SharedAccessKey</strong> part of the connection string). Then save the file.</p></li>
<li><p>Open a Command Prompt or terminal window and <code>cd</code> to the project directory. Then use the following command to run <strong>run.js</strong>:</p>
<pre><code>node run.js</code></pre></li>
<li><p>Confirm that you see output similar to the following, indicating that all 10 “cameras” are connected to the IoT hub:</p>
<pre><code>polar_cam_0003 connected
polar_cam_0005 connected
polar_cam_0001 connected
polar_cam_0009 connected
polar_cam_0004 connected
polar_cam_0006 connected
polar_cam_0008 connected
polar_cam_0007 connected
polar_cam_0002 connected
polar_cam_0010 connected</code></pre>
<p>The order in which the cameras connect to the IoT hub will probably differ from what’s shown here, and will also vary from one run to the next.</p></li>
<li><p>After a few seconds, additional output should appear. Each line corresponds to an event transmitted from a camera to the IoT hub. The output will look something like this:</p>
<pre><code>polar_cam_0008: https://streaminglabstorage.blob.core.windows.net/photos/image_24.jpg
polar_cam_0004: https://streaminglabstorage.blob.core.windows.net/photos/image_10.jpg
polar_cam_0005: https://streaminglabstorage.blob.core.windows.net/photos/image_26.jpg
polar_cam_0007: https://streaminglabstorage.blob.core.windows.net/photos/image_27.jpg
polar_cam_0001: https://streaminglabstorage.blob.core.windows.net/photos/image_15.jpg
polar_cam_0007: https://streaminglabstorage.blob.core.windows.net/photos/image_20.jpg
polar_cam_0003: https://streaminglabstorage.blob.core.windows.net/photos/image_18.jpg
polar_cam_0005: https://streaminglabstorage.blob.core.windows.net/photos/image_21.jpg
polar_cam_0001: https://streaminglabstorage.blob.core.windows.net/photos/image_20.jpg
polar_cam_0009: https://streaminglabstorage.blob.core.windows.net/photos/image_26.jpg</code></pre></li>
<li><p>Confirm that the cameras are running and generating events as shown above. Then return to the Azure Function in the portal and check the output log. Verify that over the course of a few minutes, the log shows several outputs from Stream Analytics. (The frequency will vary because the cameras use random timers to fire events.) Note that each output contains a JSON payload containing a device ID, latitude, longitude, blob URL, and timestamp.</p>
<blockquote>
<p>Remember that the Stream Analytics job doesn’t forward every event it receives to the function. It generates an output <em>only</em> when one camera snaps two photos within 10 seconds.</p>
</blockquote>
<p><img src="images/2018-05-21-15-18-34.png" /></p></li>
<li><p>Return to the Stream Analytics job in the portal and click <strong>Stop</strong> to stop it. Then click <strong>Yes</strong> when asked to confirm that you want to stop the job.</p></li>
<li><p>Return to the Command Prompt or terminal window in which <strong>run.js</strong> is running and press <strong>Ctrl+C</strong> (<strong>Command-C</strong> on a Mac) to stop it, and therefore stop the flow of events from the simulated cameras.</p></li>
</ol>
<p>You have confirmed that Stream Analytics is receiving input from the IoT hub and that the Azure Function is receiving input from Stream Analytics. In the next exercise, you will build a machine-learning model and invoke it from the Azure Function.</p>
<h2 id="summary">Summary</h2>
<p>In this exercise, you created a Stream Analytics job, connected it to an IoT hub for input, and connected it to an Azure Function for output. You also learned how Stream Analytics queries are used to extract information from temporal data streams.</p>
<hr />
<p>Copyright 2018 Microsoft Corporation. All rights reserved. Except where otherwise noted, these materials are licensed under the terms of the MIT License. You may use them according to the license as is most appropriate for your project. The terms of this license can be found at https://opensource.org/licenses/MIT.</p>
</body>
</html>
